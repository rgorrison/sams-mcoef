{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook with Python code to generate data for MCEOF analysis of a multi-proxy network for paper: South American Summer Monsoon variability over the last millennium in paleoclimate records and isotope-enabled climate models.\n",
    "\n",
    "- Imports formatted age control and sampled d18O records\n",
    "- Merges records to create standardized time series covering the period with annual resolution\n",
    "- Performs Monte Carlo resampling to generate a proxy ensemble with 1,000 members\n",
    "- EOF calculation and plotting (eof patterns and principal components)\n",
    "- Saves output as CSV for EOF analysis and plotting applications\n",
    "\n",
    "___\n",
    "\n",
    "Rebecca Orrison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/network/rit/lab/vuillelab_rit/orrison/bin/miniconda3/lib/python3.7/site-packages/xarray/core/merge.py:17: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)\n",
      "/network/rit/lab/vuillelab_rit/orrison/bin/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/network/rit/lab/vuillelab_rit/orrison/bin/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/network/rit/lab/vuillelab_rit/orrison/bin/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/network/rit/lab/vuillelab_rit/orrison/bin/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/network/rit/lab/vuillelab_rit/orrison/bin/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/network/rit/lab/vuillelab_rit/orrison/bin/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/network/rit/lab/vuillelab_rit/orrison/bin/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/network/rit/lab/vuillelab_rit/orrison/bin/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/network/rit/lab/vuillelab_rit/orrison/bin/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/network/rit/lab/vuillelab_rit/orrison/bin/miniconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "#----------\n",
    "# system\n",
    "#----------\n",
    "import os\n",
    "\n",
    "#----------\n",
    "#additional packages\n",
    "#----------\n",
    "#data\n",
    "from itertools import dropwhile\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import xarray as xr\n",
    "\n",
    "# computation\n",
    "from eofs.standard import Eof\n",
    "from eofs.xarray import Eof\n",
    "import scipy\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "from scipy import stats\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.multivariate.pca import PCA   # this is more of a climate perspective on PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# plotting\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter,LatitudeFormatter\n",
    "import cartopy.feature as cf\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_ind_fn(latl,latu):\n",
    "    \"\"\"Find the index of the latitude that corresponds to proxy latitude\"\"\"\n",
    "    return np.where((lat >= latl) & (lat <= latu))\n",
    "    \n",
    "def lon_ind_fn(lonl,lonu):\n",
    "    \"\"\"Find the index of the longitude that corresponds to proxy longitude\"\"\"\n",
    "    return np.where((lon >= lonl) & (lon <= lonu))\n",
    "\n",
    "def nn_idx(target,array):\n",
    "    return np.abs(array - target).argmin()\n",
    "\n",
    "def GapFill(year,yng,old):\n",
    "    slope = (records_dat[yng]['d18O'].iloc[-1] - records_dat[old]['d18O'][0]) / (xnew_ages[yng][j][[-1]][0] - xnew_ages[old][j][[0]][0])\n",
    "    b = (xnew_ages[yng][j][[-1]][0]*records_dat[old]['d18O'][0] - xnew_ages[old][j][[0]][0]*records_dat[yng]['d18O'].iloc[-1])/(xnew_ages[yng][j][[-1]][0]-xnew_ages[old][j][[0]][0])\n",
    "    return (slope * year) + b\n",
    "\n",
    "def smonotonic(x):\n",
    "    \"\"\"Check if the array is strictly monotonic (incr or dec)\"\"\"\n",
    "    dx = np.diff(x)\n",
    "    return np.all(dx < 0) or np.all(dx > 0) \n",
    "\n",
    "def force_smonoton(x):\n",
    "    \"\"\"Forces an array that is not strictly monotonic to conform to strict monotonicity\"\"\"\n",
    "    for i in range(len(x)-1):\n",
    "        if x[i] >= x[i+1]:\n",
    "            x[i+1] = x[i] + 0.01\n",
    "    return x\n",
    "#----------\n",
    "\n",
    "class DataDict(dict):\n",
    "    \"\"\" data_dict template \"\"\"\n",
    "    def __init__(self):         # create object\n",
    "        self[\"record\"] = []\n",
    "        self[\"oxy_depth_mm\"] = []\n",
    "        self[\"year_CE\"] = []\n",
    "        self[\"'d18O'\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "- age tie files\n",
    "- isotope time series files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {}\n",
    "pathi = \"/network/rit/home/ro553136/orrison/data/proxy/mceof_recs/\"\n",
    "for file in os.listdir(pathi):\n",
    "    if file.endswith('_ages.txt'):\n",
    "        rec = os.path.splitext(file)[0].split('_')[0]\n",
    "        records[rec] = pd.read_csv(os.path.join(pathi, file), sep='\\t')\n",
    "                \n",
    "records_dat = {}\n",
    "pathi = \"/network/rit/home/ro553136/orrison/data/proxy/mceof_recs/\"\n",
    "\n",
    "for file in os.listdir(pathi):\n",
    "    if file.endswith('_d18O.txt'):\n",
    "        rec = os.path.splitext(file)[0].split('_')[0]\n",
    "        records_dat[rec] = pd.read_csv(os.path.join(pathi, file), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Sample compositing and Monte Carlo ensemble generation\n",
    "- Monte Carlo resampling of age ties based on Gaussian uncertainty distribution\n",
    "- Standard interpolation to annual resolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------\n",
    "# Define constants\n",
    "#----------\n",
    "size = 1000    # ensemble size\n",
    "gauss_window = 30 \n",
    "hlfwnd = gauss_window // 2\n",
    "ages_comm = np.arange(850, 1850)    # last millennium time period\n",
    "ages_comm = ages_comm[::-1]\n",
    "lw_bnd = np.min(ages_comm) - hlfwnd\n",
    "up_bnd = np.max(ages_comm) + hlfwnd\n",
    "\n",
    "    # Final records names for EOF analysis\n",
    "mceof_recs = ['PAL', 'HUA1', 'PAR', 'DV2', 'SBE+SMT', 'TMO', 'CRT1',\n",
    "               'JAR', 'ALH', 'BOTO', 'PIM4', 'MV', 'QUELC3', 'PUM12']\n",
    "\n",
    "#----------\n",
    "# Pre-define arrays for data storage\n",
    "#----------\n",
    "age_mod_mc = {}\n",
    "f_mc = {}\n",
    "xnew = {}\n",
    "x = {}\n",
    "xnew_ages = {}\n",
    "annages = {}\n",
    "d18O_resamp = {}\n",
    "d18O_comm = {}\n",
    "d18O_comm_smth30 = {}\n",
    "d18O_comm_smth10 = {}\n",
    "rec_ages_list = []\n",
    "y_list = []\n",
    "\n",
    "#----------\n",
    "# Framework for data analysis\n",
    "#----------\n",
    "for rec in records: \n",
    "        age_mod_mc[rec] = [[0.0 for i in range(len(records[rec]['tie_year_CE']))] for j in range(size)]\n",
    "        xnew[rec] = np.linspace(np.min(records[rec]['tie_depth_mm']), np.max(records[rec]['tie_depth_mm']), np.max(records[rec]['tie_year_CE'])-np.min(records[rec]['tie_year_CE']))\n",
    "        f_mc[rec] = [[0.0 for i in range(len(xnew[rec]))] for j in range(size)]\n",
    "        x[rec] =  records[rec]['tie_depth_mm']\n",
    "        xnew_ages[rec] = []\n",
    "        d18O_resamp[rec] = [[None for i in range(len(xnew_ages[rec]))] for j in range(size)]    \n",
    "        d18O_comm[rec] = [[None for i in range(len(ages_comm))] for j in range(size)] \n",
    "        d18O_comm_smth30[rec] = [[None for i in range(len(ages_comm))] for j in range(size)] \n",
    "        d18O_comm_smth10[rec] = [[None for i in range(len(ages_comm))] for j in range(size)] \n",
    "\n",
    "#----------\n",
    "# Framework for records from merged samples and samples without an age model\n",
    "#----------\n",
    "newrecs = ['PUM12', 'SBE3' , 'QUELC3', 'JAR', 'PAR', 'SBE+SMT', 'PAL', 'BOTO', 'ALH', 'MV', 'PIM4']\n",
    "for nrec in newrecs:\n",
    "    xnew_ages.update({nrec : []})\n",
    "    d18O_resamp.update({nrec : [[None for i in range(len(xnew_ages[rec]))] for j in range(size)] })\n",
    "    d18O_comm.update({nrec : [[None for i in range(len(ages_comm))] for j in range(size)] })\n",
    "    d18O_comm_smth10.update({nrec : [[None for i in range(up_bnd - lw_bnd)] for j in range(size)] })\n",
    "    d18O_comm_smth30.update({nrec : [[None for i in range(up_bnd - lw_bnd)] for j in range(size)] })\n",
    "    \n",
    "#----------\n",
    "# Framework for final records \n",
    "#----------  \n",
    "for rec in mceof_recs:\n",
    "       annages[rec] = []\n",
    "#----------\n",
    "# Age ties and d18O sample interpolation --> annually spaced 18O time series\n",
    "#----------   \n",
    "for j in range(size):\n",
    "#     print('AGE MODEL RESAMPLING FOR ENS MEM ' + str(j))\n",
    "    for rec in records:\n",
    "        bol_flag = [False]*(len(records[rec]['tie_year_CE']))\n",
    "            # resample age ties, make sure they don't violate superposition assumption\n",
    "        for i in range(len(records[rec]['tie_year_CE'])):\n",
    "    #             print('AGE TIE (#/total):')\n",
    "    #             print(i, len(records[rec]['tie_year_CE']))\n",
    "            while not bol_flag[i]:\n",
    "                if i != len(records[rec]['tie_year_CE'])-1:\n",
    "                    # generate new age tie resamples based on uncertainties in one of three ways: \n",
    "                    # check if second age tie violates superposition assump of the first age tie based on lower bounds\n",
    "                    #if j is last age tie, don't do the below step.  indent the below.  \n",
    "                    tie_min_diff = (records[rec]['tie_year_CE'][i]-records[rec]['err_2sig'][i]/2) - (records[rec]['tie_year_CE'][i+1]-records[rec]['err_2sig'][i+1]/2)\n",
    "                    tie_max_diff = (records[rec]['tie_year_CE'][i]+records[rec]['err_2sig'][i]/2) - (records[rec]['tie_year_CE'][i+1]+records[rec]['err_2sig'][i+1]/2)\n",
    "                    if tie_min_diff < 0:\n",
    "                # generate truncated normal distribution of current age tie based on next one\n",
    "                        lower = (records[rec]['tie_year_CE'][i+1]-records[rec]['err_2sig'][i+1]/2)+1\n",
    "                        upper = records[rec]['tie_year_CE'][i]+records[rec]['err_2sig'][i]/2\n",
    "                        mu = records[rec]['tie_year_CE'][i]\n",
    "                        sigma = records[rec]['err_2sig'][i]/8\n",
    "                        age_mod_mc[rec][j][i] = float(scipy.stats.truncnorm.rvs((lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma,size=1))\n",
    "                    elif tie_max_diff < 0:\n",
    "                # generate truncated normal distribution of next age tie based on current one. \n",
    "                        lower = records[rec]['tie_year_CE'][i]-records[rec]['err_2sig'][i]/2\n",
    "                        upper = (records[rec]['tie_year_CE'][i]+records[rec]['err_2sig'][i]/2)+1\n",
    "                        mu = records[rec]['tie_year_CE'][i]\n",
    "                        sigma = records[rec]['err_2sig'][i]/8\n",
    "                        age_mod_mc[rec][j][i] = float(scipy.stats.truncnorm.rvs((lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma,size=1))\n",
    "                    else:\n",
    "                        age_mod_mc[rec][j][i] = float(np.random.normal(loc=records[rec]['tie_year_CE'][i], scale=(records[rec]['err_2sig'][i]/8), size = 1))\n",
    "                else:\n",
    "                    age_mod_mc[rec][j][i] = float(np.random.normal(loc=records[rec]['tie_year_CE'][i], scale=(records[rec]['err_2sig'][i]/8), size = 1))\n",
    "    #                 print()\n",
    "    #                 print('age tie year +/- 1 sig uncertainty is')\n",
    "    #                 print(records[rec]['tie_year_CE'][i], (records[rec]['err_2sig'][i]/2))\n",
    "    #                 print('age tie resamp value is')\n",
    "    #                 print(age_mod_mc[rec][j][i])\n",
    "    #                 print('previous age tie value is')\n",
    "    #                 print(age_mod_mc[rec][j][i-1])\n",
    "    #                 print(np.shape(age_mod_mc[rec]))\n",
    "    #                 print()\n",
    "                if i == 0:\n",
    "                    bol_flag[0] = True\n",
    "    #                     print(bol_flag)\n",
    "                elif i == len(records[rec]['tie_year_CE'])-1:\n",
    "                    bol_flag[-1] = True\n",
    "    #                     print(bol_flag)\n",
    "                else: \n",
    "                    if (age_mod_mc[rec][j][i] - age_mod_mc[rec][j][i-1]) < -2: # verify sufficient separation of age ties\n",
    "                        bol_flag[i] = True\n",
    "    #                         print(bol_flag)\n",
    "                    else:\n",
    "                        print('resample age tie')\n",
    "                        print(rec)\n",
    "\n",
    "#                print('FINISHED AN AGE MODEL!')\n",
    "\n",
    "        ### Generate function to calculate one age per isotopic sample depth\n",
    "        ### new interpolated ages based on sampled depths and MC-derived age model\n",
    "        f = interpolate.interp1d(records[rec]['tie_depth_mm'], age_mod_mc[rec][j][:], fill_value = \"extrapolate\")       \n",
    "        xnew_ages[rec].append(np.array(f(records_dat[rec]['oxy_depth_mm'])))\n",
    "\n",
    "#----------\n",
    "# Clean up records, merge\n",
    "#---------- \n",
    "\n",
    "#----- QUELC3\n",
    "    xnew_ages['QUELC3'].append(records_dat['QUELC3']['year_CE'].values)\n",
    "    d18O_resamp['QUELC3'][j][:] = records_dat['QUELC3']['d18O'].values \n",
    "    \n",
    "#----- PUM12 \n",
    "        ###  Annual records (exact yrs)\n",
    "    pum12ann_yr = records_dat['PUM12an']['year_CE'].values\n",
    "    pum12ann_dO = records_dat['PUM12an']['d18O'].values\n",
    "    \n",
    "        ###  Varved records, irregular sampling - interpolate to exact\n",
    "    pum12vrv_yr = records_dat['PUM12v']['year_CE']\n",
    "    pum12vrv_dO = records_dat['PUM12v']['d18O']\n",
    "    pum12vrv_f = interpolate.interp1d(pum12vrv_yr,pum12vrv_dO, fill_value='extrapolate')\n",
    "    pum12vrvann_yr = np.arange(840,1797) # input values for exact ann. interp of varved non-exact section\n",
    "    pum12vrvann_yr = pum12vrvann_yr[::-1]\n",
    "    pum12vrvann_dO = pum12vrv_f(pum12vrvann_yr)\n",
    "\n",
    "        ### merge and reset PUM12 record \n",
    "    xnew_ages['PUM12'].append(np.concatenate([pum12ann_yr, pum12vrvann_yr]))\n",
    "    d18O_resamp['PUM12'][j] = np.concatenate([pum12ann_dO, pum12vrvann_dO])\n",
    "    \n",
    "#----- ALH6\n",
    "    alh6_irreg_yr = records_dat['ALH6']['year_CE']\n",
    "    alh6_irreg_dO = records_dat['ALH6']['d18O']\n",
    "    alh6_reg_f = interpolate.interp1d(alh6_irreg_yr, alh6_irreg_dO, fill_value='extrapolate')\n",
    "    alh6_reg_d18O = alh6_reg_f(ages_comm)\n",
    "    \n",
    "        ### reset ALH6 record \n",
    "    xnew_ages['ALH'].append(ages_comm)\n",
    "    d18O_resamp['ALH'][j] = alh6_reg_d18O\n",
    "    \n",
    "#---------- \n",
    "# Fill hiatus, merging samples to build composite records\n",
    "#---------- \n",
    "#----- SBE3 a, b gap -> SBE3a + SBE3b = SBE3 \n",
    "    gap_yrs = np.arange(xnew_ages['SBE3b'][j][0],xnew_ages['SBE3a'][j][-1])\n",
    "    gap_yrs = gap_yrs[1:][::-1]\n",
    "    gap_d18O = [GapFill(year,'SBE3a','SBE3b') for year in gap_yrs]\n",
    "    ### Simple average of stdev 30 years before and after gap\n",
    "    up_bnd = nn_idx(xnew_ages['SBE3a'][j][-1]+30, xnew_ages['SBE3a'][j])\n",
    "    lw_bnd = nn_idx(xnew_ages['SBE3b'][j][0]-30, xnew_ages['SBE3b'][j])\n",
    "    stda = np.std(records_dat['SBE3a']['d18O'][up_bnd:])\n",
    "    stdb = np.std(records_dat['SBE3b']['d18O'][1:lw_bnd+1])\n",
    "    std = (stda + stdb) / 2   \n",
    "\n",
    "        ### resample and scale values within above std dev, shifting mean based on line.\n",
    "    gap_d18O_resamp = np.concatenate([np.random.normal(loc=val,scale = std, size = 1) for val in gap_d18O])\n",
    "\n",
    "        ### merge and reset SBE3 values\n",
    "    xnew_ages['SBE3'].append(np.concatenate([xnew_ages['SBE3a'][j], gap_yrs, xnew_ages['SBE3b'][j]]))\n",
    "    d18O_resamp['SBE3'][j] = np.concatenate([records_dat['SBE3a']['d18O'], gap_d18O_resamp, records_dat['SBE3b']['d18O']])\n",
    "\n",
    "#----- PIM4 a, b gap -> PIM4a + PIM4b = PIM4   \n",
    "    gap_yrs = np.arange(xnew_ages['PIM4b'][j][0],xnew_ages['PIM4a'][j][-1])\n",
    "    gap_yrs_pim = gap_yrs[1:][::-1]\n",
    "    gap_d18O = [GapFill(year,'PIM4a','PIM4b') for year in gap_yrs_pim]\n",
    "    ### Simple average of stdev 30 years before and after gap\n",
    "    up_bnd = nn_idx(xnew_ages['PIM4a'][j][-1]+30, xnew_ages['PIM4a'][j])\n",
    "    lw_bnd = nn_idx(xnew_ages['PIM4b'][j][0]-30, xnew_ages['PIM4b'][j])\n",
    "    stda = np.std(records_dat['PIM4a']['d18O'][up_bnd:])\n",
    "    stdb = np.std(records_dat['PIM4b']['d18O'][1:lw_bnd+1])\n",
    "    std = (stda + stdb) / 2   \n",
    "\n",
    "        ### resample and scale values within above std dev, shifting mean based on line.\n",
    "    gap_d18O_resamp = np.concatenate([np.random.normal(loc=val,scale = std, size = 1) for val in gap_d18O])\n",
    "\n",
    "        ### merge and reset PIM4 values\n",
    "    xnew_ages['PIM4'].append(np.concatenate([xnew_ages['PIM4a'][j], gap_yrs_pim, xnew_ages['PIM4b'][j]]))\n",
    "    d18O_resamp['PIM4'][j] = np.concatenate([records_dat['PIM4a']['d18O'], gap_d18O_resamp, records_dat['PIM4b']['d18O']])\n",
    "\n",
    "######     \n",
    "#----- MV \n",
    "    gap_over = xnew_ages['MV30'][j][[0]]+1 - xnew_ages['MV1'][j][[-1]]\n",
    "\n",
    "    ts1_d18 = np.array(records_dat['MV1']['d18O'])\n",
    "    ts1_yr = np.array(xnew_ages['MV1'][j])\n",
    "    ts2_d18 = np.array(records_dat['MV30']['d18O'])\n",
    "    ts2_yr = np.array(xnew_ages['MV30'][j])\n",
    "\n",
    "    if gap_over < 0: # gap to be filled. \n",
    "\n",
    "        strt1 = nn_idx(1850,ts1_yr)\n",
    "        end2 = nn_idx(850,ts2_yr)\n",
    "        ts1_zscr = (ts1_d18 - np.mean(ts1_d18[strt1::])) / np.std(ts1_d18[strt1::])\n",
    "        ts2_zscr = (ts2_d18 - np.mean(ts2_d18[0:end2+1])) / np.std(ts2_d18[0:end2+1])    \n",
    "\n",
    "        gap_yrs = np.arange(ts2_yr[0],ts1_yr[-1])\n",
    "        gap_yrs_mv = gap_yrs[1:][::-1]\n",
    "        slope = (ts2_zscr[0] - ts1_zscr[-1]) / (ts2_yr[[0]][0] - ts1_yr[[-1]][0])\n",
    "        b = (ts2_yr[[0]][0]*ts1_zscr[-1] - ts1_yr[[-1]][0]*ts2_zscr[0])/(ts2_yr[[0]][0]-ts1_yr[[-1]][0])\n",
    "        gap_d18O = [((slope * year) + b) for year in gap_yrs_mv]\n",
    "\n",
    "        ### Simple average of stdev 30 years before and after gap\n",
    "        up_bnd = nn_idx(xnew_ages['MV1'][j][-1]+30, xnew_ages['MV1'][j])\n",
    "        lw_bnd = nn_idx(xnew_ages['MV30'][j][0]-30, xnew_ages['MV30'][j])\n",
    "        stda = np.std(records_dat['MV1']['d18O'][up_bnd:])\n",
    "        stdb = np.std(records_dat['MV30']['d18O'][1:lw_bnd+1])\n",
    "        std = (stda + stdb) / 2    \n",
    "\n",
    "            ### resample and scale values within above std dev, shifting mean based on line.\n",
    "        gap_d18_resamp = np.concatenate([np.random.normal(loc=val,scale = std, size = 1) for val in gap_d18O])\n",
    "\n",
    "            ### merge and reset MV values, scale to variance of MV1. \n",
    "        xnew_ages['MV'].append(np.concatenate([ts1_yr, gap_yrs_mv, ts2_yr]))\n",
    "        d18_temp_comm = np.concatenate([ts1_zscr,gap_d18_resamp,ts2_zscr])\n",
    "        d18O_resamp['MV'][j] = (d18_temp_comm * np.std(ts1_d18[strt1::])) + np.mean(ts1_d18[strt1::])\n",
    "\n",
    "    elif gap_over > 2:  #  overlap exists: MERGE MV 1, 30\n",
    "\n",
    "        ts1_d18O_cubresamp_ovlp, ts2_d18O_cubresamp_ovlp = [], []\n",
    "\n",
    "        p1 = nn_idx(ts1_yr[-1],ts2_yr)   # index in ts1 for the end of overlap period.  search the time value of ts1 based on the start value of ts2\n",
    "        p2 = nn_idx(ts2_yr[0],ts1_yr)   # index in ts2 for the start of overlap period.   search the time value of ts2 based on the end value of ts1. \n",
    "\n",
    "            # find overlap of stdardized ts\n",
    "        ts1_d18_ovlp = ts1_d18[p2::]\n",
    "        ts2_d18_ovlp = ts2_d18[0:p1+1]\n",
    "\n",
    "           ### interpolate points with cubic spline. time must be monotonically increasing.\n",
    "        f_ts1_ovlp_cubic = interpolate.CubicSpline(ts1_yr[p2::][::-1], ts1_d18_ovlp[::-1], bc_type='natural') \n",
    "        f_ts2_ovlp_cubic = interpolate.CubicSpline(ts2_yr[0:p1+1][::-1], ts2_d18_ovlp[::-1], bc_type='natural') \n",
    "\n",
    "              ### Synchronize: establish annual time series covered for each rec - use years from one series, matches closest points of the other series\n",
    "              ### Set ages young -> old; round to exact years\n",
    "        sync_a = math.ceil(max(ts1_yr[p2::][-1],ts2_yr[0:p1+1][-1]))\n",
    "        sync_b = math.floor(min(ts1_yr[p2::][0],ts2_yr[0:p1+1][0]))\n",
    "        sync_ages_ovlp = np.arange(sync_a, sync_b+1, 1.)  \n",
    "\n",
    "              ### Downscale cublic spline interpolation of d18O values to annual resolution in ovlp period \n",
    "        ts1_d18O_cubresamp_ovlp[:] = f_ts1_ovlp_cubic(sync_ages_ovlp) \n",
    "        ts2_d18O_cubresamp_ovlp[:] = f_ts2_ovlp_cubic(sync_ages_ovlp) \n",
    "\n",
    "              ### replace period of overlap with synchronized section for each time series\n",
    "        ts1_d18O_full = np.concatenate([ts1_d18[0:p2], ts1_d18O_cubresamp_ovlp[::-1]])\n",
    "        ts1_yr_full = np.concatenate([ts1_yr[0:p2],sync_ages_ovlp[::-1]])\n",
    "        ts2_d18O_full = np.concatenate([ts2_d18O_cubresamp_ovlp[::-1], ts2_d18[p1+1:-1]])\n",
    "        ts2_yr_full = np.concatenate([sync_ages_ovlp[::-1], ts2_yr[p1+1:-1]])\n",
    "\n",
    "              ### standardize both records by mean, std of the records in the common period\n",
    "        strt1 = nn_idx(1850,ts1_yr)\n",
    "        end2 = nn_idx(850,ts2_yr)\n",
    "        ts1_zscr = (ts1_d18O_full - np.mean(ts1_d18O_full[strt1::])) / np.std(ts1_d18O_full[strt1::])\n",
    "        ts2_zscr = (ts2_d18O_full - np.mean(ts2_d18O_full[0:end2+1])) / np.std(ts2_d18O_full[0:end2+1])\n",
    "\n",
    "            ### Add nan to buffer; ts?_zscr must have same length\n",
    "        tmp = np.empty(len(ts2_yr[p1+1:]))\n",
    "        tmp.fill(np.nan)\n",
    "        ts1_zscr_full = np.concatenate((ts1_zscr, tmp))\n",
    "        tmp = np.empty(len(ts1_yr[0:p2]))\n",
    "        tmp.fill(np.nan)\n",
    "        ts2_zscr_full = np.concatenate((tmp, ts2_zscr))\n",
    "\n",
    "            ### average two series. Use np.nanmean so that the period with record outside overlap with nans remains\n",
    "        mv_zscr = np.nanmean( np.array([ts1_zscr_full,ts2_zscr_full]), axis = 0)\n",
    "\n",
    "        ### invert to LM mean, std values of record with longest sole LM coverage. Concatenate years together \n",
    "        d18O_resamp['MV'][j] = (mv_zscr * np.std(ts1_d18O_full[strt1::])) + np.mean(ts1_d18O_full[strt1::])\n",
    "        xnew_ages['MV'].append(np.concatenate([ts1_yr_full,ts2_yr[p1+1:-1]]))\n",
    "\n",
    "    else: #years line up perfectly or have small enough gap that spline doesn't work:\n",
    "            # still need to rescale MV30 to the std of MV1.\n",
    "        strt1 = nn_idx(1850,ts1_yr)\n",
    "        end2 = nn_idx(850,ts2_yr)\n",
    "        ts1_zscr = (ts1_d18 - np.mean(ts1_d18[strt1::])) / np.std(ts1_d18[strt1::])\n",
    "        ts2_zscr = (ts2_d18 - np.mean(ts2_d18[0:end2+1])) / np.std(ts2_d18[0:end2+1])\n",
    "\n",
    "        xnew_ages['MV'].append(np.concatenate([ts1_yr,ts2_yr]))\n",
    "        d18O_temp_comm = np.concatenate([ts1_zscr,ts2_zscr])\n",
    "\n",
    "        d18O_resamp['MV'][j] = (d18O_temp_comm * np.std(ts1_d18[strt1::])) + np.mean(ts1_d18[strt1::])                                                 \n",
    "                  \n",
    "########     \n",
    "#----- JAR \n",
    "    ts1_d18 = np.array(records_dat['JAR4']['d18O'])\n",
    "    ts1_yr = np.array(xnew_ages['JAR4'][j])\n",
    "    ts2_d18 = np.array(records_dat['JAR1']['d18O'])\n",
    "    ts2_yr = np.array(xnew_ages['JAR1'][j])\n",
    "\n",
    "    ts1_d18O_cubresamp_ovlp, ts2_d18O_cubresamp_ovlp = [], []\n",
    "\n",
    "    p1 = nn_idx(ts1_yr[-1],ts2_yr)   # index in ts1 for the end of overlap period.  search the time value of ts1 based on the start value of ts2\n",
    "    p2 = nn_idx(ts2_yr[0],ts1_yr)   # index in ts2 for the start of overlap period.   search the time value of ts2 based on the end value of ts1. \n",
    "\n",
    "    ts1_d18_ovlp = ts1_d18[p2::]\n",
    "    ts2_d18_ovlp = ts2_d18[0:p1+1]\n",
    "   \n",
    "        ### interpolate points with cubic spline. time must be monotonically increasing.\n",
    "    f_ts1_ovlp_cubic = interpolate.CubicSpline(ts1_yr[p2::][::-1], ts1_d18_ovlp[::-1], bc_type='natural') \n",
    "    f_ts2_ovlp_cubic = interpolate.CubicSpline(ts2_yr[0:p1+1][::-1], ts2_d18_ovlp[::-1], bc_type='natural') \n",
    "\n",
    "          ### Synchronize: establish annual time series covered for each rec - use years from one series, matches closest points of the other series\n",
    "          ### Set ages young -> old; round to exact years\n",
    "    sync_a = math.ceil(max(ts1_yr[p2::][-1],ts2_yr[0:p1+1][-1]))\n",
    "    sync_b = math.floor(min(ts1_yr[p2::][0],ts2_yr[0:p1+1][0]))\n",
    "    sync_ages_ovlp = np.arange(sync_a, sync_b+1, 1.)  \n",
    "\n",
    "#         ### Downscale cublic spline interpolation of d18O values to annual resolution in ovlp period \n",
    "    ts1_d18O_cubresamp_ovlp[:] = f_ts1_ovlp_cubic(sync_ages_ovlp) \n",
    "    ts2_d18O_cubresamp_ovlp[:] = f_ts2_ovlp_cubic(sync_ages_ovlp) \n",
    "    \n",
    "    ### replace period of overlap with synchronized section for each time series\n",
    "    ts1_d18O_full = np.concatenate([ts1_d18[0:p2], ts1_d18O_cubresamp_ovlp[::-1]])\n",
    "    ts1_yr_full = np.concatenate([ts1_yr[0:p2],sync_ages_ovlp[::-1]])\n",
    "    ts2_d18O_full = np.concatenate([ts2_d18O_cubresamp_ovlp[::-1], ts2_d18[p1+1:]])\n",
    "    ts2_yr_full = np.concatenate([sync_ages_ovlp[::-1], ts2_yr[p1+1:]])\n",
    "\n",
    "#     ### Standardize records (including replaced bit)) by mean, std of LM period\n",
    "    strt1 = nn_idx(1850,ts1_yr_full)\n",
    "    end2 = nn_idx(850,ts2_yr_full)\n",
    "    ts1_zscr = (ts1_d18O_full - np.mean(ts1_d18O_full[strt1::])) / np.std(ts1_d18O_full[strt1::])\n",
    "    ts2_zscr = (ts2_d18O_full - np.mean(ts2_d18O_full[0:end2+1])) / np.std(ts2_d18O_full[0:end2+1])\n",
    "\n",
    "    ### Add nan to buffer; ts?_zscr must have same length\n",
    "    tmp = np.empty(len(ts2_yr[p1+1:]))\n",
    "    tmp.fill(np.nan)\n",
    "    ts1_zscr_full = np.concatenate((ts1_zscr, tmp))\n",
    "    tmp = np.empty(len(ts1_yr[0:p2]))\n",
    "    tmp.fill(np.nan)\n",
    "    ts2_zscr_full = np.concatenate((tmp, ts2_zscr))\n",
    "\n",
    "    ### average two series. Use np.nanmean so that the period with record outside overlap with nans remains\n",
    "    jar_zscr = np.nanmean( np.array([ts1_zscr_full,ts2_zscr_full]), axis = 0)\n",
    "    \n",
    "    ### invert to LM mean, std values of record with longest sole LM coverage. Concatenate years together \n",
    "    d18O_resamp['JAR'][j] = (jar_zscr * np.std(ts1_d18O_full[strt1::])) + np.mean(ts1_d18O_full[strt1::])\n",
    "    xnew_ages['JAR'].append(np.concatenate([ts1_yr_full,ts2_yr[p1+1:]]))\n",
    "                                    \n",
    "########     \n",
    "#----- \n",
    "# PAR\n",
    "    ts1_d18 = np.array(records_dat['PAR03']['d18O'])\n",
    "    ts1_yr = np.array(xnew_ages['PAR03'][j])\n",
    "    ts2_d18 = np.array(records_dat['PAR01']['d18O'])\n",
    "    ts2_yr = np.array(xnew_ages['PAR01'][j])\n",
    "\n",
    "    ts1_d18O_cubresamp_ovlp, ts2_d18O_cubresamp_ovlp = [], []\n",
    "\n",
    "    p1 = nn_idx(ts1_yr[-1],ts2_yr)   # index in ts1 for the end of overlap period.  search the time value of ts1 based on the start value of ts2\n",
    "    p2 = nn_idx(ts2_yr[0],ts1_yr)   # index in ts2 for the start of overlap period.   search the time value of ts2 based on the end value of ts1. \n",
    "\n",
    "    ts1_d18_ovlp = ts1_d18[p2::]\n",
    "    ts2_d18_ovlp = ts2_d18[0:p1+1]\n",
    "   \n",
    "        ### interpolate points with cubic spline. time must be monotonically increasing.\n",
    "    f_ts1_ovlp_cubic = interpolate.CubicSpline(ts1_yr[p2::][::-1], ts1_d18_ovlp[::-1], bc_type='natural') \n",
    "    f_ts2_ovlp_cubic = interpolate.CubicSpline(ts2_yr[0:p1+1][::-1], ts2_d18_ovlp[::-1], bc_type='natural') \n",
    "\n",
    "          ### Synchronize: establish annual time series covered for each rec - use years from one series, matches closest points of the other series\n",
    "          ### Set ages young -> old; round to exact years\n",
    "    sync_a = math.ceil(max(ts1_yr[p2::][-1],ts2_yr[0:p1+1][-1]))\n",
    "    sync_b = math.floor(min(ts1_yr[p2::][0],ts2_yr[0:p1+1][0]))\n",
    "    sync_ages_ovlp = np.arange(sync_a, sync_b+1, 1.)  \n",
    "\n",
    "#         ### Downscale cublic spline interpolation of d18O values to annual resolution in ovlp period \n",
    "    ts1_d18O_cubresamp_ovlp[:] = f_ts1_ovlp_cubic(sync_ages_ovlp) \n",
    "    ts2_d18O_cubresamp_ovlp[:] = f_ts2_ovlp_cubic(sync_ages_ovlp) \n",
    "    \n",
    "    ### replace period of overlap with synchronized section for each time series\n",
    "    ts1_d18O_full = np.concatenate([ts1_d18[0:p2], ts1_d18O_cubresamp_ovlp[::-1]])\n",
    "    ts1_yr_full = np.concatenate([ts1_yr[0:p2],sync_ages_ovlp[::-1]])\n",
    "    ts2_d18O_full = np.concatenate([ts2_d18O_cubresamp_ovlp[::-1], ts2_d18[p1+1:]])\n",
    "    ts2_yr_full = np.concatenate([sync_ages_ovlp[::-1], ts2_yr[p1+1:]])\n",
    "\n",
    "#     ### Standardize records (including replaced bit)) by mean, std of LM period\n",
    "    strt1 = nn_idx(1850,ts1_yr_full)\n",
    "    end2 = nn_idx(850,ts2_yr_full)\n",
    "    ts1_zscr = (ts1_d18O_full - np.mean(ts1_d18O_full[strt1::])) / np.std(ts1_d18O_full[strt1::])\n",
    "    ts2_zscr = (ts2_d18O_full - np.mean(ts2_d18O_full[0:end2+1])) / np.std(ts2_d18O_full[0:end2+1])\n",
    "\n",
    "    ### Add nan to buffer; ts?_zscr must have same length\n",
    "    tmp = np.empty(len(ts2_yr[p1+1:]))\n",
    "    tmp.fill(np.nan)\n",
    "    ts1_zscr_full = np.concatenate((ts1_zscr, tmp))\n",
    "    tmp = np.empty(len(ts1_yr[0:p2]))\n",
    "    tmp.fill(np.nan)\n",
    "    ts2_zscr_full = np.concatenate((tmp, ts2_zscr))\n",
    "\n",
    "    ### average two series. Use np.nanmean so that the period with record outside overlap with nans remains\n",
    "    par_zscr = np.nanmean( np.array([ts1_zscr_full,ts2_zscr_full]), axis = 0)\n",
    "    \n",
    "    ### invert to LM mean, std values of record with longest sole LM coverage. Concatenate years together \n",
    "    d18O_resamp['PAR'][j] = (par_zscr * np.std(ts1_d18O_full[strt1::])) + np.mean(ts1_d18O_full[strt1::])\n",
    "    xnew_ages['PAR'].append(np.concatenate([ts1_yr_full,ts2_yr[p1+1:]]))\n",
    "    \n",
    "#----- \n",
    "# SBE+SMT\n",
    "    ts1_d18 = np.array(d18O_resamp['SBE3'][j])\n",
    "    ts1_yr = np.array(xnew_ages['SBE3'][j])\n",
    "    ts2_d18 = np.array(records_dat['SMT5']['d18O'])\n",
    "    ts2_yr = np.array(xnew_ages['SMT5'][j])\n",
    "\n",
    "    ts1_d18O_cubresamp_ovlp, ts2_d18O_cubresamp_ovlp = [], []\n",
    "\n",
    "    p1 = nn_idx(ts1_yr[-1],ts2_yr)   # index in ts1 for the end of overlap period.  search the time value of ts1 based on the start value of ts2\n",
    "    p2 = nn_idx(ts2_yr[0],ts1_yr)   # index in ts2 for the start of overlap period.   search the time value of ts2 based on the end value of ts1. \n",
    "\n",
    "    ts1_d18_ovlp = ts1_d18[p2::]\n",
    "    ts2_d18_ovlp = ts2_d18[0:p1+1]\n",
    "   \n",
    "        ### interpolate points with cubic spline. time must be monotonically increasing.\n",
    "    f_ts1_ovlp_cubic = interpolate.CubicSpline(ts1_yr[p2::][::-1], ts1_d18_ovlp[::-1], bc_type='natural') \n",
    "    f_ts2_ovlp_cubic = interpolate.CubicSpline(ts2_yr[0:p1+1][::-1], ts2_d18_ovlp[::-1], bc_type='natural') \n",
    "\n",
    "          ### Synchronize: establish annual time series covered for each rec - use years from one series, matches closest points of the other series\n",
    "          ### Set ages young -> old; round to exact years\n",
    "    sync_a = math.ceil(max(ts1_yr[p2::][-1],ts2_yr[0:p1+1][-1]))\n",
    "    sync_b = math.floor(min(ts1_yr[p2::][0],ts2_yr[0:p1+1][0]))\n",
    "    sync_ages_ovlp = np.arange(sync_a, sync_b+1, 1.)  \n",
    "\n",
    "#         ### Downscale cublic spline interpolation of d18O values to annual resolution in ovlp period \n",
    "    ts1_d18O_cubresamp_ovlp[:] = f_ts1_ovlp_cubic(sync_ages_ovlp) \n",
    "    ts2_d18O_cubresamp_ovlp[:] = f_ts2_ovlp_cubic(sync_ages_ovlp) \n",
    "    \n",
    "    ### replace period of overlap with synchronized section for each time series\n",
    "    ts1_d18O_full = np.concatenate([ts1_d18[0:p2], ts1_d18O_cubresamp_ovlp[::-1]])\n",
    "    ts1_yr_full = np.concatenate([ts1_yr[0:p2],sync_ages_ovlp[::-1]])\n",
    "    ts2_d18O_full = np.concatenate([ts2_d18O_cubresamp_ovlp[::-1], ts2_d18[p1+1:]])\n",
    "    ts2_yr_full = np.concatenate([sync_ages_ovlp[::-1], ts2_yr[p1+1:]])\n",
    "\n",
    "#     ### Standardize records (including replaced bit)) by mean, std of LM period\n",
    "    strt1 = nn_idx(1850,ts1_yr_full)\n",
    "    end2 = nn_idx(850,ts2_yr_full)\n",
    "    ts1_zscr = (ts1_d18O_full - np.mean(ts1_d18O_full[strt1::])) / np.std(ts1_d18O_full[strt1::])\n",
    "    ts2_zscr = (ts2_d18O_full - np.mean(ts2_d18O_full[0:end2+1])) / np.std(ts2_d18O_full[0:end2+1])\n",
    "\n",
    "    ### Add nan to buffer; ts?_zscr must have same length\n",
    "    tmp = np.empty(len(ts2_yr[p1+1:]))\n",
    "    tmp.fill(np.nan)\n",
    "    ts1_zscr_full = np.concatenate((ts1_zscr, tmp))\n",
    "    tmp = np.empty(len(ts1_yr[0:p2]))\n",
    "    tmp.fill(np.nan)\n",
    "    ts2_zscr_full = np.concatenate((tmp, ts2_zscr))\n",
    "\n",
    "    ### average two series. Use np.nanmean so that the period with record outside overlap with nans remains\n",
    "    sbesmt_zscr = np.nanmean( np.array([ts1_zscr_full,ts2_zscr_full]), axis = 0)\n",
    "    \n",
    "    ### invert to LM mean, std values of record with longest sole LM coverage. Concatenate years together \n",
    "    d18O_resamp['SBE+SMT'][j] = (sbesmt_zscr * np.std(ts1_d18O_full[strt1::])) + np.mean(ts1_d18O_full[strt1::])\n",
    "    xnew_ages['SBE+SMT'].append(np.concatenate([ts1_yr_full,ts2_yr[p1+1:]]))\n",
    "    \n",
    "#----- \n",
    "# PAL3 and PAL4.  PAL 3 as a base. \n",
    "    ts1_d18 = np.array(records_dat['PAL03']['d18O'])\n",
    "    ts1_yr = np.array(xnew_ages['PAL03'][j])\n",
    "    ts2_d18 = np.array(records_dat['PAL04']['d18O'])\n",
    "    ts2_yr = np.array(xnew_ages['PAL04'][j])\n",
    "\n",
    "    ts1_d18O_cubresamp_ovlp, ts2_d18O_cubresamp_ovlp = [], []\n",
    "\n",
    "    p1 = nn_idx(ts1_yr[-1],ts2_yr)   # index in ts1 for the end of overlap period.  search the time value of ts1 based on the start value of ts2\n",
    "    p2 = nn_idx(ts2_yr[0],ts1_yr)   # index in ts2 for the start of overlap period.   search the time value of ts2 based on the end value of ts1. \n",
    "\n",
    "    ts1_d18_ovlp = ts1_d18[p2::]\n",
    "    ts2_d18_ovlp = ts2_d18[0:p1+1]\n",
    "   \n",
    "        ### interpolate points with cubic spline. time must be monotonically increasing.\n",
    "    f_ts1_ovlp_cubic = interpolate.CubicSpline(ts1_yr[p2::][::-1], ts1_d18_ovlp[::-1], bc_type='natural') \n",
    "    f_ts2_ovlp_cubic = interpolate.CubicSpline(ts2_yr[0:p1+1][::-1], ts2_d18_ovlp[::-1], bc_type='natural') \n",
    "\n",
    "          ### Synchronize: establish annual time series covered for each rec - use years from one series, matches closest points of the other series\n",
    "          ### Set ages young -> old; round to exact years\n",
    "    sync_a = math.ceil(max(ts1_yr[p2::][-1],ts2_yr[0:p1+1][-1]))\n",
    "    sync_b = math.floor(min(ts1_yr[p2::][0],ts2_yr[0:p1+1][0]))\n",
    "    sync_ages_ovlp = np.arange(sync_a, sync_b+1, 1.)  \n",
    "\n",
    "#         ### Downscale cublic spline interpolation of d18O values to annual resolution in ovlp period \n",
    "    ts1_d18O_cubresamp_ovlp[:] = f_ts1_ovlp_cubic(sync_ages_ovlp) \n",
    "    ts2_d18O_cubresamp_ovlp[:] = f_ts2_ovlp_cubic(sync_ages_ovlp) \n",
    "    \n",
    "    ### replace period of overlap with synchronized section for each time series\n",
    "    ts1_d18O_full = np.concatenate([ts1_d18[0:p2], ts1_d18O_cubresamp_ovlp[::-1]])\n",
    "    ts1_yr_full = np.concatenate([ts1_yr[0:p2],sync_ages_ovlp[::-1]])\n",
    "    ts2_d18O_full = np.concatenate([ts2_d18O_cubresamp_ovlp[::-1], ts2_d18[p1+1:]])\n",
    "    ts2_yr_full = np.concatenate([sync_ages_ovlp[::-1], ts2_yr[p1+1:]])\n",
    "\n",
    "#     ### Standardize records (including replaced bit)) by mean, std of LM period\n",
    "    strt1 = nn_idx(1850,ts1_yr_full)\n",
    "    end2 = nn_idx(850,ts2_yr_full)\n",
    "    ts1_zscr = (ts1_d18O_full - np.mean(ts1_d18O_full[strt1::])) / np.std(ts1_d18O_full[strt1::])\n",
    "    ts2_zscr = (ts2_d18O_full - np.mean(ts2_d18O_full[0:end2+1])) / np.std(ts2_d18O_full[0:end2+1])\n",
    "\n",
    "    ### Add nan to buffer; ts?_zscr must have same length\n",
    "    tmp = np.empty(len(ts2_yr[p1+1:]))\n",
    "    tmp.fill(np.nan)\n",
    "    ts1_zscr_full = np.concatenate((ts1_zscr, tmp))\n",
    "    tmp = np.empty(len(ts1_yr[0:p2]))\n",
    "    tmp.fill(np.nan)\n",
    "    ts2_zscr_full = np.concatenate((tmp, ts2_zscr))\n",
    "\n",
    "    ### average two series. Use np.nanmean so that the period with record outside overlap with nans remains\n",
    "    pal_zscr = np.nanmean( np.array([ts1_zscr_full,ts2_zscr_full]), axis = 0)\n",
    "    \n",
    "    ### invert to LM mean, std values of record with longest sole LM coverage. Concatenate years together \n",
    "    d18O_resamp['PAL'][j] = (pal_zscr * np.std(ts1_d18O_full[strt1::])) + np.mean(ts1_d18O_full[strt1::])\n",
    "    xnew_ages['PAL'].append(np.concatenate([ts1_yr_full,ts2_yr[p1+1:]]))\n",
    "\n",
    "#----- \n",
    "# BOTO.  Boto 3 as base.  \n",
    " \n",
    "    ts1_d18 = np.array(records_dat['BOTO7']['d18O'])\n",
    "    ts1_yr = np.array(xnew_ages['BOTO7'][j])\n",
    "    ts2_d18 = np.array(records_dat['BOTO3']['d18O'])\n",
    "    ts2_yr = np.array(xnew_ages['BOTO3'][j])\n",
    "    ts3_d18 = np.array(records_dat['BOTO10']['d18O'])\n",
    "    ts3_yr = np.array(xnew_ages['BOTO10'][j])\n",
    "    ts4_d18 = np.array(records_dat['BOTO1']['d18O'])\n",
    "    ts4_yr = np.array(xnew_ages['BOTO1'][j])\n",
    "    \n",
    "    # MERGING BOTO3,BOTO7 into BOTOa\n",
    "\n",
    "    ts1_d18O_cubresamp_ovlp, ts2_d18O_cubresamp_ovlp = [], []\n",
    "\n",
    "    p1 = nn_idx(ts1_yr[-1],ts2_yr)   # index in ts1 for the end of overlap period.  search the time value of ts1 based on the start value of ts2\n",
    "    p2 = nn_idx(ts2_yr[0],ts1_yr)   # index in ts2 for the start of overlap period.   search the time value of ts2 based on the end value of ts1. \n",
    "\n",
    "    ts1_d18_ovlp = ts1_d18[p2::]\n",
    "    ts2_d18_ovlp = ts2_d18[0:p1+1]\n",
    "   \n",
    "        ### interpolate points with cubic spline. time must be monotonically increasing.\n",
    "    f_ts1_ovlp_cubic = interpolate.CubicSpline(ts1_yr[p2::][::-1], ts1_d18_ovlp[::-1], bc_type='natural') \n",
    "    f_ts2_ovlp_cubic = interpolate.CubicSpline(ts2_yr[0:p1+1][::-1], ts2_d18_ovlp[::-1], bc_type='natural') \n",
    "\n",
    "          ### Synchronize: establish annual time series covered for each rec - use years from one series, matches closest points of the other series\n",
    "          ### Set ages young -> old; round to exact years\n",
    "    sync_a = math.ceil(max(ts1_yr[p2::][-1],ts2_yr[0:p1+1][-1]))\n",
    "    sync_b = math.floor(min(ts1_yr[p2::][0],ts2_yr[0:p1+1][0]))\n",
    "    sync_ages_ovlp = np.arange(sync_a, sync_b+1, 1.)  \n",
    "\n",
    "#         ### Downscale cublic spline interpolation of d18O values to annual resolution in ovlp period \n",
    "    ts1_d18O_cubresamp_ovlp[:] = f_ts1_ovlp_cubic(sync_ages_ovlp) \n",
    "    ts2_d18O_cubresamp_ovlp[:] = f_ts2_ovlp_cubic(sync_ages_ovlp) \n",
    "    \n",
    "    ### replace period of overlap with synchronized section for each time series\n",
    "    ts1_d18O_full = np.concatenate([ts1_d18[0:p2], ts1_d18O_cubresamp_ovlp[::-1]])\n",
    "    ts1_yr_full = np.concatenate([ts1_yr[0:p2],sync_ages_ovlp[::-1]])\n",
    "    ts2_d18O_full = np.concatenate([ts2_d18O_cubresamp_ovlp[::-1], ts2_d18[p1+1:]])\n",
    "    ts2_yr_full = np.concatenate([sync_ages_ovlp[::-1], ts2_yr[p1+1:]])\n",
    "\n",
    "#     ### Standardize records (including replaced bit)) by mean, std of LM period\n",
    "    strt1 = nn_idx(1850,ts1_yr_full)\n",
    "    end2 = nn_idx(850,ts2_yr_full)\n",
    "    ts1_zscr = (ts1_d18O_full - np.mean(ts1_d18O_full[strt1::])) / np.std(ts1_d18O_full[strt1::])\n",
    "    ts2_zscr = (ts2_d18O_full - np.mean(ts2_d18O_full[0:end2+1])) / np.std(ts2_d18O_full[0:end2+1])\n",
    "\n",
    "    ### Add nan to buffer; ts?_zscr must have same length\n",
    "    tmp1 = np.empty(len(ts2_yr[p1+1:]))\n",
    "    tmp1.fill(np.nan)\n",
    "    ts1_zscr_full = np.concatenate((ts1_zscr, tmp1))\n",
    "    tmp2 = np.empty(len(ts1_yr[0:p2]))\n",
    "    tmp2.fill(np.nan)\n",
    "    ts2_zscr_full = np.concatenate((tmp2, ts2_zscr))\n",
    "\n",
    "    ### average two series. Use np.nanmean so that the period with record outside overlap with nans remains\n",
    "    botoa_zscr = np.nanmean( np.array([ts1_zscr_full,ts2_zscr_full]), axis = 0)\n",
    "    \n",
    "    botoa_d18_invrt = (botoa_zscr * np.std(ts2_d18O_full[0:end2+1])) + np.mean(ts2_d18O_full[0:end2+1])\n",
    "    botoa_yr = np.concatenate([ts1_yr_full,ts2_yr[p1+1:]])\n",
    "                                                                              \n",
    "    # MERGING BOTOa,BOTO10\n",
    "\n",
    "    tsa_d18O_cubresamp_ovlp, ts3_d18O_cubresamp_ovlp = [], []\n",
    "\n",
    "    p3 = nn_idx(botoa_yr[-1],ts3_yr)   # index in ts1 for the end of overlap period.  search the time value of ts1 based on the start value of ts2\n",
    "    p4 = nn_idx(ts3_yr[0],botoa_yr)   # index in ts2 for the start of overlap period.   search the time value of ts2 based on the end value of ts1. \n",
    "\n",
    "    botoa_d18_ovlp = botoa_d18_invrt[p4::]   \n",
    "    ts3_d18_ovlp = ts3_d18[0:p3+1]     \n",
    "   \n",
    "        ### interpolate points with cubic spline. time must be monotonically increasing.\n",
    "    f_tsa_ovlp_cubic = interpolate.CubicSpline(botoa_yr[p4::][::-1], botoa_d18_ovlp[::-1], bc_type='natural') \n",
    "    f_ts3_ovlp_cubic = interpolate.CubicSpline(ts3_yr[0:p3+1][::-1], ts3_d18_ovlp[::-1], bc_type='natural') \n",
    "\n",
    "          ### Synchronize: establish annual time series covered for each rec - use years from one series, matches closest points of the other series\n",
    "          ### Set ages young -> old; round to exact years\n",
    "    sync_a = math.ceil(max(botoa_yr[p4::][-1],ts3_yr[0:p3+1][-1]))\n",
    "    sync_b = math.floor(min(botoa_yr[p4::][0],ts3_yr[0:p3+1][0]))\n",
    "    sync_ages_ovlp = np.arange(sync_a, sync_b+1, 1.)  \n",
    "\n",
    "#         ### Downscale cublic spline interpolation of d18O values to annual resolution in ovlp period \n",
    "    tsa_d18O_cubresamp_ovlp[:] = f_tsa_ovlp_cubic(sync_ages_ovlp) \n",
    "    ts3_d18O_cubresamp_ovlp[:] = f_ts3_ovlp_cubic(sync_ages_ovlp) \n",
    "    \n",
    "    ### replace period of overlap with synchronized section for each time series\n",
    "    tsa_d18O_full = np.concatenate([botoa_d18_invrt[0:p4], tsa_d18O_cubresamp_ovlp[::-1]])\n",
    "    tsa_yr_full = np.concatenate([botoa_yr[0:p4],sync_ages_ovlp[::-1]])\n",
    "    ts3_d18O_full = np.concatenate([ts3_d18O_cubresamp_ovlp[::-1],ts3_d18[p3+1:]])\n",
    "    ts3_yr_full = np.concatenate([sync_ages_ovlp[::-1], ts3_yr[p3+1:]])\n",
    "\n",
    "#     ### Standardize records (including replaced bit)) by mean, std of LM period\n",
    "    strt2 = nn_idx(1850,tsa_yr_full)\n",
    "    end3 = nn_idx(850,ts3_yr_full)                                                                          \n",
    "    tsa_zscr = (tsa_d18O_full - np.mean(tsa_d18O_full[strt2::])) / np.std(tsa_d18O_full[strt2::])\n",
    "    ts3_zscr = (ts3_d18O_full - np.mean(ts3_d18O_full[0:end3+1])) / np.std(ts3_d18O_full[0:end3+1])\n",
    "\n",
    "    ### Add nan to buffer; ts?_zscr must have same length\n",
    "    tmpa = np.empty(len(ts3_yr[p3+1:]))\n",
    "    tmpa.fill(np.nan)\n",
    "    tsa_zscr_full = np.concatenate([tsa_zscr, tmpa])\n",
    "    tmp3 = np.empty(len(botoa_yr[0:p4]))\n",
    "    tmp3.fill(np.nan)\n",
    "    ts3_zscr_full = np.concatenate([tmp3, ts3_zscr])\n",
    "\n",
    "    ### average two series. Use np.nanmean so that the period with record outside overlap with nans remains\n",
    "    botob_zscr = np.nanmean( np.array([tsa_zscr_full,ts3_zscr_full]), axis = 0)\n",
    "    \n",
    "    botob_d18_invrt = (botob_zscr * np.std(tsa_d18O_full[strt1::])) + np.mean(tsa_d18O_full[strt1::])\n",
    "    botob_yr = np.concatenate([tsa_yr_full,ts3_yr[p3+1:]])\n",
    "        \n",
    "    # MERGING BOTOb,BOTO1, Botob as base, boto1 embedded in it.\n",
    "    \n",
    "    tsb_d18O_cubresamp_ovlp, ts4_d18O_cubresamp_ovlp = [], []\n",
    "\n",
    "    p5 = nn_idx(ts4_yr[-1],botob_yr)   # index in ts4 for the end of overlap period.  search the time value of botob based on the start value of ts2\n",
    "    p6 = nn_idx(ts4_yr[0],botob_yr)   # index in ts4 for the start of overlap period.   search the time value of botob based on the end value of ts1. \n",
    "\n",
    "    tsb_d18_ovlp = botob_d18_invrt[p6:p5+1]\n",
    "    ts4_d18_ovlp = ts4_d18[:]\n",
    "   \n",
    "        ### interpolate points with cubic spline. time must be monotonically increasing.\n",
    "    f_tsb_ovlp_cubic = interpolate.CubicSpline(botob_yr[p6:p5+1][::-1], tsb_d18_ovlp[::-1], bc_type='natural') \n",
    "    f_ts4_ovlp_cubic = interpolate.CubicSpline(ts4_yr[::-1], ts4_d18_ovlp[::-1], bc_type='natural') \n",
    "\n",
    "          ### Synchronize: establish annual time series covered for each rec - use years from one series, matches closest points of the other series\n",
    "          ### Set ages young -> old; round to exact years\n",
    "    sync_a = math.ceil(max(botob_yr[p6:p5+1][-1],ts4_yr[0:p1+1][-1]))\n",
    "    sync_b = math.floor(min(botob_yr[p6:p5+1][0],ts4_yr[0:p1+1][0]))\n",
    "    sync_ages_ovlp = np.arange(sync_a, sync_b+1, 1.)  \n",
    "\n",
    "#         ### Downscale cublic spline interpolation of d18O values to annual resolution in ovlp period \n",
    "    tsb_d18O_cubresamp_ovlp[:] = f_tsb_ovlp_cubic(sync_ages_ovlp) \n",
    "    ts4_d18O_cubresamp_ovlp[:] = f_ts4_ovlp_cubic(sync_ages_ovlp) \n",
    "    \n",
    "    ### replace period of overlap with synchronized section for each time series\n",
    "    tsb_d18O_full = np.concatenate([botob_d18_invrt[0:p6], tsb_d18O_cubresamp_ovlp[::-1],botob_d18_invrt[p5+1::]])\n",
    "    tsb_yr_full = np.concatenate([botob_yr[0:p6],sync_ages_ovlp[::-1], botob_yr[p5+1::]])\n",
    "    ts4_d18O_full = np.concatenate([ts4_d18O_cubresamp_ovlp[::-1]])\n",
    "    ts4_yr_full = np.concatenate([sync_ages_ovlp[::-1]])\n",
    "\n",
    "#     ### Standardize records (including replaced bit)) by mean, std of LM period\n",
    "    strt4 = nn_idx(1850,tsb_yr_full)\n",
    "    end5 = nn_idx(850,tsb_yr_full)\n",
    "    tsb_zscr = (tsb_d18O_full - np.mean(tsb_d18O_full[strt4:end5+1])) / np.std(tsb_d18O_full[strt4:end5+1])\n",
    "    ts4_zscr = (ts4_d18O_full - np.mean(ts4_d18O_full)) / np.std(ts4_d18O_full)\n",
    "\n",
    "    ### Add nan to buffer; ts?_zscr must have same length\n",
    "    tsb_zscr_full = tsb_zscr\n",
    "    tmp4a = np.empty(len(botob_yr[0:p6]))\n",
    "    tmp4a.fill(np.nan)\n",
    "    tmp4b = np.empty(len(botob_yr[p5+1::]))\n",
    "    tmp4b.fill(np.nan)\n",
    "    ts4_zscr_full = np.concatenate((tmp4a, ts4_zscr, tmp4b))\n",
    "\n",
    "    ### average two series. Use np.nanmean so that the period with record outside overlap with nans remains\n",
    "    botoc_zscr = np.nanmean( np.array([tsb_zscr_full,ts4_zscr_full]), axis = 0)\n",
    "\n",
    "    ### invert to LM mean, std values of record with longest sole LM coverage. Concatenate years together \n",
    "    d18O_resamp['BOTO'][j] = (botoc_zscr * np.std(tsb_d18O_full[strt4:end5+1])) + np.mean(tsb_d18O_full[strt4:end5+1])\n",
    "    xnew_ages['BOTO'].append(tsb_yr_full)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------- \n",
    "# Generate new function for annual interpolation of d18O values. \n",
    "#----------     \n",
    "for j in range(size):\n",
    "    for rec in mceof_recs:\n",
    "        if rec in ['HUA1', 'DV2', 'TMO', 'CRT1']:\n",
    "            f_d18O_ann_linear = interpolate.interp1d(xnew_ages[rec][j], records_dat[rec]['d18O'], fill_value = \"extrapolate\") \n",
    "        else: \n",
    "            f_d18O_ann_linear = interpolate.interp1d(xnew_ages[rec][j], d18O_resamp[rec][j], fill_value = \"extrapolate\") \n",
    "\n",
    "        ### Establish annual time series covered for each rec, each ensemble member\n",
    "        ### Set ages young -> old; round to exact years\n",
    "        xnew_ages_tmp = np.arange(np.min(xnew_ages[rec][j]), np.max(xnew_ages[rec][j])+1, 1.)\n",
    "        xnew_ages_tmp = np.around(xnew_ages_tmp[::-1])  \n",
    "        annages[rec].append(xnew_ages_tmp)\n",
    "\n",
    "        ### Linear interpolation of d18O values to annual resolution\n",
    "        d18O_resamp[rec][j] = f_d18O_ann_linear(annages[rec][j]) \n",
    "\n",
    "    #---------- \n",
    "    # Truncate dataset to 850 -- 1850 CE\n",
    "    #---------- \n",
    "\n",
    "        p1 = nn_idx(np.max(ages_comm),annages[rec][j])\n",
    "        p2 = nn_idx(np.min(ages_comm),annages[rec][j])\n",
    "        d18O_comm[rec][j] = d18O_resamp[rec][j][p1:p2+1]         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build data frame, save output of data as abs values and z scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute vals: \n",
    "abs_prox_all = []\n",
    "for iter in range(size):\n",
    "    abs_prox = pd.DataFrame(d18O_comm[mceof_recs[0]][iter], index = ages_comm, columns=[mceof_recs[0]])\n",
    "    for i in range(len(mceof_recs)-1):\n",
    "        abs_prox.insert(i+1, value=(d18O_comm[mceof_recs[i+1]][iter]), column=mceof_recs[i+1])\n",
    "  \n",
    "    abs_prox_all.append(abs_prox)\n",
    "    #print(iter)\n",
    "    #print(abs_prox)\n",
    "    \n",
    "f = open(\"/network/rit/lab/vuillelab_rit/orrison/data/proxy/mceof_recs/mceof_recs_all_Sept21.csv\",\"a\")\n",
    "for df in abs_prox_all:\n",
    "    df.to_csv(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z scores: \n",
    "zmat_prox_all = []\n",
    "for iter in range(size):\n",
    "    zmat_prox = pd.DataFrame(stats.zscore(d18O_comm[mceof_recs[0]][iter]), index = ages_comm, columns=[mceof_recs[0]])\n",
    "    for i in range(len(mceof_recs)-1):\n",
    "        zmat_prox.insert(i+1, value=stats.zscore(d18O_comm[mceof_recs[i+1]][iter]), column=mceof_recs[i+1])\n",
    "    \n",
    "    zmat_prox_all.append(zmat_prox)\n",
    "    #print(iter)\n",
    "    #print(zmat_prox)\n",
    "\n",
    "f = open(\"/network/rit/lab/vuillelab_rit/orrison/data/proxy/mceof_recs/mceof_zscores_all_Septv21.csv\",\"a\")\n",
    "for df in zmat_prox_all:\n",
    "    df.to_csv(f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
